{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>input_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>0</td>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>0</td>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>0</td>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text  label  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...      0   \n",
       "1  House Intelligence Committee Chairman Devin Nu...      0   \n",
       "2  On Friday, it was revealed that former Milwauk...      0   \n",
       "3  On Christmas day, Donald Trump announced that ...      0   \n",
       "4  Pope Francis used his annual Christmas Day mes...      0   \n",
       "\n",
       "                                          input_text  \n",
       "0   Donald Trump Sends Out Embarrassing New Year’...  \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...  \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...  \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...  \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#df = pd.read_csv(\"balanced_news_dataset.csv\")\n",
    "df = pd.read_csv(\"fakeandTrue_news_dataset.csv\")\n",
    "# Combine 'title' and 'text' into a single column for input\n",
    "df[\"input_text\"] = df[\"title\"] + \" \" + df[\"text\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nisha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Split data (60% train, 30% test, 10% validation)\\ntrain_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\\ntest_df, val_df = train_test_split(temp_df, test_size=0.25, random_state=42)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    if pd.isna(text):  # Handle NaN values\n",
    "        return None\n",
    "    # Remove URLs only\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Keep case, numbers, @mentions, #hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s@#0-9]', '', text)  \n",
    "    return text.strip()  # No lowercasing/stopword removal\n",
    "\n",
    "df.head()\n",
    "df['cleaned_text'] = df['input_text'].apply(preprocess_text)\n",
    "df = df[df['cleaned_text'].str.split().str.len() >= 10]  # Step 1: Filter short texts\n",
    "\n",
    "\"\"\"# Split data (60% train, 30% test, 10% validation)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "test_df, val_df = train_test_split(temp_df, test_size=0.25, random_state=42)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['cleaned_text'], keep='first')\n",
    "# --- NEW: SUBSET SAMPLING ---\n",
    "\"\"\"desired_total = 5000  # Set your target size here\n",
    "if len(df) > desired_total:\n",
    "    df, _ = train_test_split(\n",
    "        df,\n",
    "        train_size=desired_total,\n",
    "        random_state=42,\n",
    "        stratify=df['label']\n",
    "    )\n",
    "\n",
    "# --- Proceed with splits ---\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.3, \n",
    "    random_state=42,\n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, \n",
    "    test_size=0.5, \n",
    "    random_state=42,\n",
    "    stratify=temp_df['label']\n",
    ")\"\"\"\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.4, #0.3\n",
    "    random_state=42,\n",
    "    stratify=df['label']  # Preserve class distribution\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, \n",
    "    test_size=0.25, #0.5\n",
    "    random_state=42,\n",
    "    stratify=temp_df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "60427\n",
      "Train-Val Overlap: 0\n",
      "Train-Val Overlap: 0\n",
      "Train-Test Overlap: 0\n",
      "40285 20142 40285\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Verify no overlap\n",
    "train_texts = set(train_df['cleaned_text'])\n",
    "val_texts = set(val_df['cleaned_text'])\n",
    "test_texts = set(test_df['cleaned_text'])\n",
    "print(len(train_texts & val_texts))\n",
    "print(len(train_df['cleaned_text'])+ len(val_df['cleaned_text']))\n",
    "print(f\"Train-Val Overlap: {len(train_texts & val_texts)}\")  # Should be 0\n",
    "print(f\"Train-Val Overlap: {len(train_texts & val_texts)}\")\n",
    "print(f\"Train-Test Overlap: {len(train_texts & test_texts)}\")  # Should be 0\n",
    "print(len(train_texts ),  len(val_texts),len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc9b3b0398d4af9aef896a04c24ea17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440209a07ceb4d7385b87426e63d7acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975413def63c49dd92c85793a31289f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4f7a2c43f04b13bafeb49b2903ce32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5ff32a26034eaea63ed0b846240238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fefe4069de49fdbfdcd717328ad1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertModel\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Initialize fast tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    padding='max_length',     # ENFORCE fixed-length\n",
    "    truncation=True,          # Force truncation\n",
    "    max_length=128,\n",
    "    return_tensors='pt'   ,    # Direct tensor conversion\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"def process_data(df):\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Tokenize text\n",
    "    dataset = dataset.map(\n",
    "        lambda x: tokenizer(\n",
    "            x['cleaned_text'],\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        ),\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        remove_columns=['cleaned_text']\n",
    "    )\n",
    "    \n",
    "    # Convert labels\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {'labels': torch.tensor(x['label'], dtype=torch.long)},\n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    # Set tensor format with labels\n",
    "    dataset.set_format(\n",
    "        type='torch',\n",
    "        columns=['input_ids', 'attention_mask', 'token_type_ids', 'labels'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\"\"\"\n",
    "# 1. MODIFIED DATA PROCESSING (Keep data on CPU)\n",
    "def process_data(df):\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Tokenize on CPU\n",
    "    dataset = dataset.map(\n",
    "        lambda x: tokenizer(\n",
    "            x['cleaned_text'],\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        ),\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        remove_columns=['cleaned_text']\n",
    "    )\n",
    "    \n",
    "    # Keep labels on CPU\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {'labels': x['label']},  # Keep as int, not tensor\n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    # Set format to CPU tensors\n",
    "    dataset.set_format(\n",
    "        type='torch',\n",
    "        columns=['input_ids', 'attention_mask', 'labels'],\n",
    "        device='cpu'  # CRITICAL CHANGE\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# 4. Process all splits\n",
    "train_dataset = process_data(train_df)\n",
    "val_dataset = process_data(val_df)\n",
    "test_dataset = process_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_dataset(dataset):\n",
    "    print(\"Feature lengths:\")\n",
    "    print(f\"Input IDs: {dataset[0]['input_ids'].shape}\")\n",
    "    print(f\"Attention Mask: {dataset[0]['attention_mask'].shape}\")\n",
    "    print(f\"Labels: {dataset[0]['labels'] if 'labels' in dataset.features else 'No labels'}\")\n",
    "\n",
    "    lengths = [len(x['input_ids']) for x in dataset]\n",
    "    assert len(set(lengths)) == 1, \"Varying sequence lengths detected!\"\n",
    "    print(\"All sequences have length:\", lengths[0])\n",
    "\n",
    "verify_dataset(train_dataset)\n",
    "verify_dataset(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# ------------------------------------\n",
    "# 3. Custom BERT Model (3-layer CLS concat)\n",
    "# ------------------------------------\n",
    "\"\"\"class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.bert.config.output_hidden_states = True  # Enable selectively\n",
    "        \n",
    "        # Calculate hidden_size once\n",
    "        hidden_size = bert_model.config.hidden_size\n",
    "        self.cls_layer = nn.Linear(3 * hidden_size, 2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Get only last 3 layers' hidden states\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=[-3, -2, -1],  # Layer indices -3, -2, -1\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Simplified CLS extraction\n",
    "        hidden_states = outputs.hidden_states  # Tuple of 3 layers\n",
    "        cls_embeddings = torch.cat([\n",
    "            hidden_states[0][:, 0, :],  # -3rd layer\n",
    "            hidden_states[1][:, 0, :],  # -2nd\n",
    "            hidden_states[2][:, 0, :]   # -1st\n",
    "        ], dim=1)\n",
    "        \n",
    "        logits = self.cls_layer(cls_embeddings)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        \n",
    "        return {'loss': loss, 'logits': logits}  # Return dict for .loss access\n",
    "\n",
    "# Initialize with optimized settings\n",
    "bert_model = BertModel.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    output_hidden_states=False  # Controlled in forward\n",
    ")\n",
    "model = BERTClassifier(bert_model).to(device)\n",
    "\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ===============\n",
    "# 2. Model Setup\n",
    "# ===============\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.bert.config.output_hidden_states = True\n",
    "        \n",
    "        hidden_size = 768  # Fixed for bert-base-uncased\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(3 * hidden_size, 2).float()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None,**kwargs):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Last 3 transformer layers (indices 10,11,12)\n",
    "        hidden_states = outputs.hidden_states[-3:]  \n",
    "        # Validate layer selection\n",
    "        assert len(hidden_states) == 3, f\"Expected 3 layers, got {len(hidden_states)}\"\n",
    "        for hs in hidden_states:\n",
    "            assert hs.size(-1) == 768, f\"Invalid hidden size {hs.size()}\"\n",
    "        # CLS tokens with FP32 casting\n",
    "        cls_embeddings = torch.cat([\n",
    "            hidden_states[0][:, 0, :].float(),  # Layer 10\n",
    "            hidden_states[1][:, 0, :].float(),  # Layer 11\n",
    "            hidden_states[2][:, 0, :].float()   # Layer 12\n",
    "        ], dim=1)\n",
    "        \n",
    "        pooled = self.dropout(cls_embeddings)\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        loss = self.loss_fn(logits, labels) if labels is not None else None\n",
    "            \n",
    "        return {'loss': loss, 'logits': logits, 'embeddings': cls_embeddings}\n",
    "    \n",
    "  \n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model = BERTClassifier(bert_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40285\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if f\"encoder.layer.{i}.\" in n],\n",
    "        \"lr\": 5e-5 * (0.9 ** (11 - i))  # Gradual decay\n",
    "    } for i in range(12)\n",
    "] + [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if \"cls_layer\" in n],\n",
    "        \"lr\": 1e-4  # Slightly reduced\n",
    "    }\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Debug async ops\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'backend:cudaMallocAsync'  # AMD fix\n",
    "\n",
    "# Force synchronous mode\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cuda.sdp_kernel(enable_flash=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to start of training loop\n",
    "dummy_input = tokenizer(\"This is a test news article.\", return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**dummy_input)\n",
    "    print(f\"Dummy CLS3 shape: {outputs['embeddings'].shape}\")\n",
    "# Expected: torch.Size([1, 2304])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Add this at the start of your code\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Enable TensorFloat-32\n",
    "torch.backends.cudnn.benchmark = True  # Auto-optimize convolution algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nisha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/6715 [00:00<?, ?it/s]c:\\Users\\nisha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 6715/6715 [09:05<00:00, 12.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.1689 | Val Acc: 97.47%\n"
     ]
    }
   ],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "# Config (Paper-Aligned with GPU Constraints)\n",
    "batch_size = 6  # Max your GPU can handle\n",
    "\"\"\"effective_batch = 24  # Paper uses 24 (6*4 but scaled to your GPU)\n",
    "grad_accum_steps = effective_batch // batch_size  # 3 steps\"\"\"\n",
    "grad_accum_steps = 4\n",
    "max_lr = 2e-5  # As per paper\n",
    "num_epochs = 1  # Must stay at 1 epoch\n",
    "mixed_precision = True\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    #pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "   # pin_memory=True\n",
    ")\n",
    "# ======================\n",
    "# 3. Optimizer & Scheduler\n",
    "# ======================\n",
    "optimizer = AdamW([\n",
    "    {'params': model.bert.parameters(), 'lr': 2e-5},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
    "], weight_decay=0.01)\n",
    "\n",
    "total_steps = len(train_loader) // grad_accum_steps\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.3,\n",
    "    div_factor=25.0,\n",
    "    final_div_factor=100.0\n",
    ")\n",
    "\n",
    "# ==================\n",
    "# 4. Training Loop\n",
    "# ==================\n",
    "scaler = GradScaler(\n",
    "    enabled=mixed_precision,\n",
    "    growth_interval=2000\n",
    "    )\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "        with autocast(device_type='cuda', dtype=torch.float16,enabled=mixed_precision):\n",
    "            # Memory-optimized transfer\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].contiguous().to(device, non_blocking=True),\n",
    "                'attention_mask': batch['attention_mask'].contiguous().to(device, non_blocking=True),\n",
    "                'labels': batch['labels'].contiguous().to(device, non_blocking=True)\n",
    "            }\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs['loss'] / grad_accum_steps\n",
    "        \n",
    "        # Backprop with scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        if (batch_idx + 1) % grad_accum_steps == 0:\n",
    "            # Gradient management\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # Reduced from 1.0\n",
    "            \n",
    "            # Optimizer step\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            torch.cuda.synchronize()\n",
    "            # LR scheduling\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Memory cleanup\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        total_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad(), autocast(device_type='cuda', dtype=torch.float16,enabled=mixed_precision):\n",
    "        for val_batch in val_loader:\n",
    "            inputs = {\n",
    "                'input_ids': val_batch['input_ids'].contiguous().to(device),\n",
    "                'attention_mask': val_batch['attention_mask'].contiguous().to(device),\n",
    "                'labels': val_batch['labels'].contiguous().to(device)\n",
    "            }\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            val_loss += outputs['loss'].item()\n",
    "            \n",
    "            # Precision metrics\n",
    "            preds = torch.argmax(outputs['logits'], dim=1)\n",
    "            val_correct += (preds == inputs['labels']).sum().item()\n",
    "            val_total += inputs['labels'].size(0)\n",
    "    \n",
    "    # Statistics\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Save best\n",
    "    if val_acc > best_acc:\n",
    "        torch.save(model.state_dict(), 'bert_cls3.pth')\n",
    "        best_acc = val_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,  # From paper's evaluation section\n",
    "    shuffle=False,  # Critical for proper evaluation\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 6715/6715 [02:02<00:00, 54.83it/s]\n",
      "Extracting embeddings: 100%|██████████| 630/630 [00:30<00:00, 20.36it/s]\n",
      "Extracting embeddings: 100%|██████████| 105/105 [00:10<00:00, 10.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Step 5: Extract CLS3 embeddings (Eq.5)\n",
    "# --------------------------------------------------\n",
    "def extract_embeddings(model, dataloader, device='cuda'):\n",
    "    \"\"\"Generate contextualized embeddings using CLS3 concatenation\"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad(), autocast(device_type='cuda', dtype=torch.float16):\n",
    "        for batch in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device, non_blocking=True),\n",
    "                'attention_mask': batch['attention_mask'].to(device, non_blocking=True)\n",
    "            }\n",
    "            \n",
    "            # Forward pass (returns dict with 'embeddings')\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Paper's Eq.5: concat(H_{CLS}^L, H_{CLS}^{L-1}, H_{CLS}^{L-2})\n",
    "            embeddings = outputs['embeddings'].cpu().numpy().astype(np.float32)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            \n",
    "            all_embeddings.append(embeddings)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    return np.concatenate(all_embeddings), np.concatenate(all_labels)\n",
    "\n",
    "# Run extraction\n",
    "train_embs, train_labels = extract_embeddings(model, train_loader)\n",
    "val_embs, val_labels = extract_embeddings(model, val_loader)\n",
    "test_embs, test_labels = extract_embeddings(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no overlap between splits\n",
    "train_texts = set(train_df['cleaned_text'])\n",
    "val_texts = set(val_df['cleaned_text'])\n",
    "test_texts = set(test_df['cleaned_text'])\n",
    "\n",
    "assert len(train_texts & val_texts) == 0, \"Train-Val leakage!\"\n",
    "assert len(train_texts & test_texts) == 0, \"Train-Test leakage!\"\n",
    "assert len(val_texts & test_texts) == 0, \"Val-Test leakage!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-21 21:15:36,226] A new study created in memory with name: no-name-a0428a6e-6c80-440c-896e-f8470498c867\n",
      "[I 2025-03-21 21:17:45,375] Trial 0 finished with value: 0.023830801310694073 and parameters: {'num_leaves': 33, 'lr': 0.015195139950383966, 'min_child_samples': 40, 'subsample': 0.9233480114710293, 'colsample_bytree': 0.731938676589442, 'reg_alpha': 0.057427965448657324}. Best is trial 0 with value: 0.023830801310694073.\n",
      "[I 2025-03-21 21:20:37,277] Trial 1 finished with value: 0.023483268791579782 and parameters: {'num_leaves': 37, 'lr': 0.05293451641664748, 'min_child_samples': 10, 'subsample': 0.9966715057368878, 'colsample_bytree': 0.7799620554978535, 'reg_alpha': 0.001708555587204413}. Best is trial 1 with value: 0.023483268791579782.\n",
      "[I 2025-03-21 21:22:32,844] Trial 2 finished with value: 0.02363221129977162 and parameters: {'num_leaves': 30, 'lr': 0.01826582823697512, 'min_child_samples': 30, 'subsample': 0.7443647262769147, 'colsample_bytree': 0.7505251809895565, 'reg_alpha': 0.0036131282851139656}. Best is trial 1 with value: 0.023483268791579782.\n",
      "[I 2025-03-21 21:24:58,415] Trial 3 finished with value: 0.023483268791579782 and parameters: {'num_leaves': 33, 'lr': 0.034693806797174785, 'min_child_samples': 49, 'subsample': 0.8306897079083274, 'colsample_bytree': 0.9278861013734554, 'reg_alpha': 0.0018642485960567456}. Best is trial 1 with value: 0.023483268791579782.\n",
      "[I 2025-03-21 21:27:27,543] Trial 4 finished with value: 0.023383973786118558 and parameters: {'num_leaves': 36, 'lr': 0.0780626847225917, 'min_child_samples': 15, 'subsample': 0.9915034115039925, 'colsample_bytree': 0.724763439167777, 'reg_alpha': 0.01728342959794464}. Best is trial 4 with value: 0.023383973786118558.\n",
      "[I 2025-03-21 21:27:29,032] Trial 5 pruned. Trial was pruned at iteration 13.\n",
      "[I 2025-03-21 21:27:30,893] Trial 6 pruned. Trial was pruned at iteration 13.\n",
      "[I 2025-03-21 21:27:31,844] Trial 7 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-03-21 21:27:32,638] Trial 8 pruned. Trial was pruned at iteration 3.\n",
      "[I 2025-03-21 21:27:33,045] Trial 9 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-03-21 21:29:26,822] Trial 10 finished with value: 0.02323503127792672 and parameters: {'num_leaves': 24, 'lr': 0.09907734207668151, 'min_child_samples': 11, 'subsample': 0.8820337476246096, 'colsample_bytree': 0.8374172744788426, 'reg_alpha': 0.014179113399561741}. Best is trial 10 with value: 0.02323503127792672.\n",
      "[I 2025-03-21 21:29:28,449] Trial 11 pruned. Trial was pruned at iteration 15.\n",
      "[I 2025-03-21 21:29:31,506] Trial 12 pruned. Trial was pruned at iteration 26.\n",
      "[I 2025-03-21 21:29:31,909] Trial 13 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-21 21:29:32,330] Trial 14 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-21 21:29:32,735] Trial 15 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-21 21:32:20,688] Trial 16 finished with value: 0.02318538377519611 and parameters: {'num_leaves': 40, 'lr': 0.08280497813009899, 'min_child_samples': 13, 'subsample': 0.8621446129973775, 'colsample_bytree': 0.8139227125871187, 'reg_alpha': 0.03486971807674323}. Best is trial 16 with value: 0.02318538377519611.\n",
      "[I 2025-03-21 21:32:21,101] Trial 17 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-21 21:32:21,628] Trial 18 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-03-21 21:32:22,060] Trial 19 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-21 21:32:29,912] Trial 20 pruned. Trial was pruned at iteration 86.\n",
      "[I 2025-03-21 21:32:30,467] Trial 21 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-03-21 21:32:31,164] Trial 22 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-03-21 21:32:31,545] Trial 23 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-21 21:32:32,764] Trial 24 pruned. Trial was pruned at iteration 6.\n",
      "[I 2025-03-21 21:32:33,907] Trial 25 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-03-21 21:35:39,526] Trial 26 finished with value: 0.02303644126700427 and parameters: {'num_leaves': 40, 'lr': 0.08195647596799933, 'min_child_samples': 21, 'subsample': 0.9029900552966483, 'colsample_bytree': 0.96395090748487, 'reg_alpha': 0.01934476318745675}. Best is trial 26 with value: 0.02303644126700427.\n",
      "[I 2025-03-21 21:35:39,960] Trial 27 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-21 21:35:40,599] Trial 28 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-03-21 21:35:41,219] Trial 29 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-03-21 21:35:41,667] Trial 30 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-21 21:35:42,393] Trial 31 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-03-21 21:38:28,384] Trial 32 finished with value: 0.022837851256081818 and parameters: {'num_leaves': 36, 'lr': 0.08653128172922714, 'min_child_samples': 13, 'subsample': 0.8823417631926579, 'colsample_bytree': 0.8632146551823339, 'reg_alpha': 0.008949561447719115}. Best is trial 32 with value: 0.022837851256081818.\n",
      "[I 2025-03-21 21:38:29,633] Trial 33 pruned. Trial was pruned at iteration 7.\n",
      "[I 2025-03-21 21:38:30,068] Trial 34 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-21 21:38:30,538] Trial 35 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-21 21:38:32,318] Trial 36 pruned. Trial was pruned at iteration 11.\n",
      "[I 2025-03-21 21:38:33,016] Trial 37 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-03-21 21:41:30,030] Trial 38 finished with value: 0.02308608876973488 and parameters: {'num_leaves': 39, 'lr': 0.07154795909014904, 'min_child_samples': 13, 'subsample': 0.8675533303442166, 'colsample_bytree': 0.8577513816251595, 'reg_alpha': 0.014482597310781518}. Best is trial 32 with value: 0.022837851256081818.\n",
      "[I 2025-03-21 21:41:30,422] Trial 39 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-21 21:41:31,555] Trial 40 pruned. Trial was pruned at iteration 5.\n",
      "[I 2025-03-21 21:44:19,810] Trial 41 finished with value: 0.02323503127792672 and parameters: {'num_leaves': 38, 'lr': 0.08658756112832047, 'min_child_samples': 13, 'subsample': 0.8511293521781158, 'colsample_bytree': 0.8305070679789757, 'reg_alpha': 0.013947083815552831}. Best is trial 32 with value: 0.022837851256081818.\n",
      "[I 2025-03-21 21:44:20,403] Trial 42 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-03-21 21:44:20,865] Trial 43 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-21 21:44:21,583] Trial 44 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-03-21 21:44:22,040] Trial 45 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-21 21:44:22,429] Trial 46 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-03-21 21:44:23,164] Trial 47 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-03-21 21:44:23,745] Trial 48 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-03-21 21:44:24,158] Trial 49 pruned. Trial was pruned at iteration 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_leaves': 36, 'lr': 0.08653128172922714, 'min_child_samples': 13, 'subsample': 0.8823417631926579, 'colsample_bytree': 0.8632146551823339, 'reg_alpha': 0.008949561447719115}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from lightgbm import Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "# Create native Dataset objects\n",
    "dtrain = Dataset(train_embs, label=train_labels, free_raw_data=False)\n",
    "dval = Dataset(val_embs, label=val_labels, reference=dtrain, free_raw_data=False)\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'boosting_type': 'dart',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 40),\n",
    "        'learning_rate': trial.suggest_float('lr', 0.01, 0.1, log=True),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 0.1, log=True),\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_error',  # Must match pruning metric\n",
    "        'verbosity': -1,\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "    }\n",
    "    \n",
    "    pruning_callback = LightGBMPruningCallback(\n",
    "        trial, \n",
    "        metric='binary_error',  # Prune based on error (lower=better)\n",
    "        valid_name='valid_0'\n",
    "    )\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        valid_sets=[dval],\n",
    "        num_boost_round=1000,\n",
    "        callbacks=[\n",
    "            pruning_callback,\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "            lgb.log_evaluation(0)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Use binary_error for consistency with minimization\n",
    "    error_rate = model.best_score['valid_0']['binary_error']\n",
    "    return error_rate  # Minimizing error\n",
    "\n",
    "study = optuna.create_study(direction='minimize')  # Change to minimize\n",
    "study.optimize(objective, n_trials=50)\n",
    "best_params = study.best_params\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's binary_error: 0.0239762\n",
      "[100]\tvalid_0's binary_error: 0.0238273\n",
      "[150]\tvalid_0's binary_error: 0.0232316\n",
      "[200]\tvalid_0's binary_error: 0.0230827\n",
      "[250]\tvalid_0's binary_error: 0.0229337\n",
      "[300]\tvalid_0's binary_error: 0.0227848\n",
      "[350]\tvalid_0's binary_error: 0.0229337\n",
      "[400]\tvalid_0's binary_error: 0.0226359\n",
      "[450]\tvalid_0's binary_error: 0.0226359\n",
      "[500]\tvalid_0's binary_error: 0.0226359\n",
      "[550]\tvalid_0's binary_error: 0.0229337\n",
      "[600]\tvalid_0's binary_error: 0.0235294\n",
      "[650]\tvalid_0's binary_error: 0.0229337\n",
      "[700]\tvalid_0's binary_error: 0.0232316\n",
      "[750]\tvalid_0's binary_error: 0.0230827\n",
      "[800]\tvalid_0's binary_error: 0.0230827\n",
      "[850]\tvalid_0's binary_error: 0.0233805\n",
      "[900]\tvalid_0's binary_error: 0.0232316\n",
      "[950]\tvalid_0's binary_error: 0.0229337\n",
      "[1000]\tvalid_0's binary_error: 0.0230827\n",
      "\n",
      "=== Paper-Comparable Results ===\n",
      "Test Accuracy: 97.69%\n",
      "F1-Score: 97.79%\n",
      "Precision: 98.25%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.97      0.98      0.98      3199\n",
      "        Fake       0.98      0.97      0.98      3516\n",
      "\n",
      "    accuracy                           0.98      6715\n",
      "   macro avg       0.98      0.98      0.98      6715\n",
      "weighted avg       0.98      0.98      0.98      6715\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x22a9dfb0250>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, classification_report\n",
    "\n",
    "# 1. Paper-specified data merging (60% train + 30% val = 90% final train)\n",
    "final_train_embs = np.concatenate([train_embs, val_embs])\n",
    "final_train_labels = np.concatenate([train_labels, val_labels])\n",
    "\n",
    "# 2. Create dataset objects with paper's split ratio\n",
    "dtrain = lgb.Dataset(final_train_embs, label=final_train_labels)\n",
    "dtest = lgb.Dataset(test_embs, label=test_labels, reference=dtrain)\n",
    "\n",
    "# 3. Final model parameters (adjusted for paper's DART boosting)\n",
    "final_params = {\n",
    "    'boosting_type': 'dart',\n",
    "    'num_leaves': 36,\n",
    "    'learning_rate': 0.0865,\n",
    "    'min_child_samples': 13,\n",
    "    'subsample': 0.8823,\n",
    "    'colsample_bytree': 0.8632,\n",
    "    'reg_alpha': 0.00895,\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_error',\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0,\n",
    "    'seed': 42,\n",
    "    'num_threads': 8  # Match your 8-core Ryzen\n",
    "}\n",
    "\n",
    "# 4. Paper's training protocol (Table 2 specs)\n",
    "final_model = lgb.train(\n",
    "    final_params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[dtest],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(50)  # Print every 50 iterations\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 5. Paper-style evaluation metrics\n",
    "test_preds = final_model.predict(test_embs) > 0.5  # Binary threshold\n",
    "\n",
    "print(\"\\n=== Paper-Comparable Results ===\")\n",
    "print(f\"Test Accuracy: {accuracy_score(test_labels, test_preds)*100:.2f}%\")\n",
    "print(f\"F1-Score: {f1_score(test_labels, test_preds)*100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(test_labels, test_preds)*100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=['Real', 'Fake']))\n",
    "\n",
    "# 6. Save model for deployment\n",
    "final_model.save_model('optimized_lightgbm.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Original Training Set ===\n",
      "                Predicted\n",
      "             |   Real   |   Fake  |\n",
      "------------------------------------\n",
      "Actual Real |   19195   |      0  |\n",
      "------------------------------------\n",
      "Actual Fake |      0   |   21090  |\n",
      "------------------------------------\n",
      "\n",
      "=== Validation Set ===\n",
      "                Predicted\n",
      "             |   Real   |   Fake  |\n",
      "------------------------------------\n",
      "Actual Real |   9597   |      0  |\n",
      "------------------------------------\n",
      "Actual Fake |      0   |   10545  |\n",
      "------------------------------------\n",
      "\n",
      "=== Test Set ===\n",
      "                Predicted\n",
      "             |   Real   |   Fake  |\n",
      "------------------------------------\n",
      "Actual Real |   3138   |     61  |\n",
      "------------------------------------\n",
      "Actual Fake |     94   |   3422  |\n",
      "------------------------------------\n",
      "\n",
      "=== Entire Combined Dataset ===\n",
      "                Predicted\n",
      "             |   Real   |   Fake  |\n",
      "------------------------------------\n",
      "Actual Real |   31930   |     61  |\n",
      "------------------------------------\n",
      "Actual Fake |     94   |   35057  |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# After model training, generate predictions for ALL splits\n",
    "train_preds = final_model.predict(train_embs) > 0.5\n",
    "val_preds = final_model.predict(val_embs) > 0.5\n",
    "test_preds = final_model.predict(test_embs) > 0.5\n",
    "\n",
    "def print_confusion(title, y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(\"                Predicted\")\n",
    "    print(\"             |   Real   |   Fake  |\")\n",
    "    print(\"------------------------------------\")\n",
    "    print(f\"Actual Real |   {cm[0,0]:4d}   |   {cm[0,1]:4d}  |\")\n",
    "    print(\"------------------------------------\")\n",
    "    print(f\"Actual Fake |   {cm[1,0]:4d}   |   {cm[1,1]:4d}  |\")\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "# Print all confusion matrices\n",
    "print_confusion(\"Original Training Set\", train_labels, train_preds)\n",
    "print_confusion(\"Validation Set\", val_labels, val_preds)\n",
    "print_confusion(\"Test Set\", test_labels, test_preds)\n",
    "\n",
    "# Optional: Combined dataset confusion matrix\n",
    "full_embs = np.concatenate([train_embs, val_embs, test_embs])\n",
    "full_labels = np.concatenate([train_labels, val_labels, test_labels])\n",
    "full_probs = final_model.predict(full_embs)\n",
    "full_preds = full_probs > 0.5\n",
    "print_confusion(\"Entire Combined Dataset\", full_labels, full_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Full Dataset Evaluation ===\n",
      "Accuracy: 99.77%\n",
      "F1-Score: 99.78%\n",
      "Precision: 99.83%\n",
      "Recall/Sensitivity: 99.73%\n",
      "ROC AUC: 99.99%\n",
      "\n",
      "Classification Report (Full Dataset):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real     0.9971    0.9981    0.9976     31991\n",
      "        Fake     0.9983    0.9973    0.9978     35151\n",
      "\n",
      "    accuracy                         0.9977     67142\n",
      "   macro avg     0.9977    0.9977    0.9977     67142\n",
      "weighted avg     0.9977    0.9977    0.9977     67142\n",
      "\n",
      "\n",
      "Normalized Confusion Matrix (Full Dataset):\n",
      "                Predicted\n",
      "             |   Real    |   Fake   |\n",
      "-------------------------------------\n",
      "Actual Real |  99.81% |   0.19% |\n",
      "-------------------------------------\n",
      "Actual Fake |   0.27% |  99.73% |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score,roc_auc_score\n",
    "# Calculate comprehensive metrics\n",
    "print(\"\\n=== Full Dataset Evaluation ===\")\n",
    "print(f\"Accuracy: {accuracy_score(full_labels, full_preds)*100:.2f}%\")\n",
    "print(f\"F1-Score: {f1_score(full_labels, full_preds)*100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(full_labels, full_preds)*100:.2f}%\") \n",
    "print(f\"Recall/Sensitivity: {recall_score(full_labels, full_preds)*100:.2f}%\")\n",
    "print(f\"ROC AUC: {roc_auc_score(full_labels, full_probs)*100:.2f}%\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report (Full Dataset):\")\n",
    "print(classification_report(full_labels, full_preds, \n",
    "                            target_names=['Real', 'Fake'],\n",
    "                            digits=4))\n",
    "\n",
    "# Confusion matrix with normalized view\n",
    "print(\"\\nNormalized Confusion Matrix (Full Dataset):\")\n",
    "cm = confusion_matrix(full_labels, full_preds)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "print(\"                Predicted\")\n",
    "print(\"             |   Real    |   Fake   |\")\n",
    "print(\"-------------------------------------\")\n",
    "print(f\"Actual Real | {cm_normalized[0,0]:>7.2%} | {cm_normalized[0,1]:>7.2%} |\")\n",
    "print(\"-------------------------------------\")\n",
    "print(f\"Actual Fake | {cm_normalized[1,0]:>7.2%} | {cm_normalized[1,1]:>7.2%} |\")\n",
    "print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Experimental Results ===\n",
      "Dataset: hehe works\n",
      "Split Ratio: 60-30-10\n",
      "Embedding Method: CLS3 Concatenation\n",
      "Classifier: LightGBM (DART)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3138   61]\n",
      " [  94 3422]]\n",
      "\n",
      "Accuracy: 97.69%\n",
      "Precision: 98.25%\n",
      "Recall: 97.33%\n",
      "F1-Score: 97.79%\n",
      "\n",
      "=== Prediction Samples ===\n",
      "| text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |   true_label |   predicted_label |   confidence |\n",
      "|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------:|------------------:|-------------:|\n",
      "| In a first Indian submarine INS Sindhukesari docks in Indonesia amid South China Sea conflict An Indian submarine has for the first time ever docked in Indonesia  which is among the countries locked in a maritime dispute with China in the contentious South China Sea as part of the continuing overall diplomaticmilitary outreach to Asean countriesThe 3000tonne dieselelectric submarine INS Sindhukesari  reached Jakarta for the maiden operational turnaround after transiting through the Sunda Strait on Wednesday \u001cIndian warships often visit Indonesia and other Asean countries This first longrange deployment of a submarine underlines the operational capability and reach of the countrys underwater combat arm as well\u001d a senior official told TOIINS Sindhukesari had undergone a major Rs 1197crore medium refitcumlife extension at Severodvinsk in Russia that ended in 2018 as part of the ongoing plan to upgrade four old Sindhughoshclass Russianorigin Kiloclass and two Shishumarclass German HDW  submarines to stem depletion in the underwater fleetThe submarines deployment to Indonesia comes soon after India conducted operational training to handle the BrahMos supersonic cruise missiles for 21 military personnel from Philippines at Nagpur earlier this monthIndia will supply three missile batteries of the shorebased antiship systems of the BrahMos a deadly conventional nonnuclear weapon that flies almost three times the speed of sound at Mach 28 with a strike range of 290km under the 375 million contract inked in January last yearThe first such BrahMos export order to Philippines which also has bitter territorial disputes with China in the South China Sea is expected to pave the way for such deals with other Asean countries like Indonesia and VietnamAlong with the push for military ties with African and Gulf countries on one side India has been steadily cranking up defence relations with Asean countries on the other side through combat exercises exchanges training programmes to operate fighters and submarines and now increasingly weapon suppliesIndia for instance had transferred a Kiloclass submarine INS Sindhuvir to Myanmar in 2020 as was then reported by TOI Apart from BrahMos jointly developed with Russia India also hopes to sell the indigenous Akash missile systems which can intercept hostile aircraft helicopters drones and subsonic cruise missiles at a range of 25km to countries like Philippines Indonesia and Vietnam among other countriesOn the bilateral exercises front India has conducted the  SIMBEX  naval exercise with Singapore the Agni Warrior Army exercise with Singapore Harimau Shakti with Malaysia and the Garud Shakti with Indonesia in recent monthsWith Indonesia India also conducts coordinated naval patrols twice a year with the last one being held along the international maritime boundary line in December\u001cIndia and Indonesia have expanded their strategic and defence cooperation in a wide range of areas especially after inking a new defence cooperation agreement during PM Narendra Modis visit to the country in 2018\u001d another official said                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |            1 |                 1 |  0.997602    |\n",
      "| Demi Lovato Takes A Stand For Trans Rights At #BBMAS And It Is GLORIOUS TWEET Musicians tend to be very left leaning Unless they re Ted Nugent usually rock stars stand up for LGBT equality and other civil rights causes That is just what superstar singer Demi Lovato did Sunday Night at the Billboard Music AwardsAwards are a very important musical event each year and it s a great place to make a statementLovato took the stage to do her performance and she wore a mesh top with protransgender bathroom symbol on it as part of her concert ensemble She didn t have to say a word that top showed the thousands of people in that audience and the millions of people at home where she stands on the issue of transgender rights The It Gets Better project tweeted a photo of Lovato as she sang at the awards showKudos to @ddlovato   taking a stand for the #LGBTQ community at the #BBMAs #genderneutral #itgetsbetter pictwittercomoLZ2uHBlQO  It Gets Better @ItGetsBetter May 23 2016This is an amazing gesture Some stars don t want to get political especially when it comes to culture wars but sometimes it is important to use one s influence to speak out for vulnerable people and to promote what s right That is what Demi Lovato and brave stars like her are doing They are well aware that they may lose fans who disagree with their stance but they do it anyway because they want to use their public platform for goodThank you Demi Lovato You are a class actFeatured image via David BeckerGetty Images                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |            0 |                 0 |  1.87018e-06 |\n",
      "| UK aid minister to resign rather than be sacked BBC cites source LONDON Reuters  British aid minister Priti Patel will resign rather than be sacked by Prime Minister Theresa May the BBC s political editor quoted an unnamed source as saying on Wednesday Patel was meeting May on Wednesday to answer questions over undisclosed meetings with Israeli officials                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |            1 |                 1 |  0.999999    |\n",
      "| Christian Dad Beat Daughter With Frozen Bacon While Quoting Bible  For Jesus It s always amazing how many folks who claim to be Christians are exactly nothing like Jesus On Tuesday an 11yearold Michigan girl burst into tears intermittently as she testified in court that her father 45yearold Jonathan A Powell  beat her with frozen bacon as he quoted the BibleIn January the girl and her sister went to stay with Powell at his home for the weekend when her father for one reason or another decided to dish out some good oldfashioned Biblical punishment He was hitting me with his elbows his upper arms and his hands  the girl said while describing an assault that began in the kitchen adding she didn t know why he had decided to beat her Her father pulled bacon from the freezer and began hitting her over the head with it He was hitting me with frozen bacon and he was pushing me back and I fell right over the dishwater I broke it He was just yelling that the dishwasher was broken The girl retreated to the bedroom but the assault continued I ran over to my room and my sister was asking me what was wrong I was in my room for a while I heard him swearing saying  You Bword you broke my dishwasher Her father began banging on her door the girl recalled as she burst into tears She explained that Powell entered her room and pulled on the bed causing her to hit her head on the wall He then  threw a tantrum  and began hitting her with his hands and arms bloodying her nose This new assault which lasted approximately 10 minutes was justified with the Bible He said  It s OK to do that it s from the Bible   if you spare the rod you spoil the child  He just said that Or in this case  bacon  rather than  the rod She says she then texted her mother and let her know she wanted to go home early but Powell refused to take her home until she finally told her mother what had been done to her The mother contacted the police who took photographs that showed injuries to the side of her facePowell is charged with one count of thirddegree child abuse for which he faces up to two years in prison   where he ll have plenty of time to pray about what he didFeatured image via Michigan Live |            0 |                 0 |  2.41117e-06 |\n",
      "| Unrest due to CAA impacted sales in December Titan Bengaluru Titan Company said sales across consumer segments were hurt due to protests against the new citizenship law that forced store closures in December\u001cSales in all divisions in the second half of December was also impacted to some extent due to forced store closures due to the protests in the North East and in many other parts of the country\u001d said the Tata Groupowned firm in a filing to the BSEThe Citizenship Amendment Act  CAA  which came into force last month has faced fierce opposition across India but has seen more unrest in few states including AssamThe company that sells the eponymous watch brand and Tanishq said it witnessed contrasting performance for each of its businesses Retail sales in jewellery were better than expected at the beginning of the quarter possibly due to a good wedding season and reasonable inelasticity of wedding jewellery but growth in watches and eyewear were difficult to come byIn the watches and wearables division the growth for the third quarter was flat compared to the previous year The company in the previous quarter had said it trimmed its FY20 guidance for revenue growth in its watches division to account for the slowdown in demand \u001cThe general economic slowdown leading to poor consumer sentiment and lower walkins has been a concern for the last few quarters\u001d Titan said in the quarterly updateLast week consumer products firm Marico in its quarterly update said consumption trends during quarter ended December belied expectations of the beginning of a revival in sentiment which were built on the back of good monsoons and announcement of various government measures                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |            1 |                 1 |  0.999978    |\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "# 5. Generate test predictions\n",
    "test_pred_probs = final_model.predict(test_embs)\n",
    "test_preds = (test_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# 6. Save predictions with original text (paper-style)\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming test_df contains original text and labels\n",
    "results_df = pd.DataFrame({\n",
    "    'text': test_df['cleaned_text'],\n",
    "    'true_label': test_labels,\n",
    "    'predicted_label': test_preds,\n",
    "    'confidence': test_pred_probs\n",
    "})\n",
    "\n",
    "# Save to CSV with paper's metrics\n",
    "results_df.to_csv('paper_metrics_predictions.csv', index=False)\n",
    "\n",
    "# 7. Paper-style formatted output\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"\\n=== Experimental Results ===\")\n",
    "print(f\"Dataset: hehe works\")\n",
    "print(f\"Split Ratio: 60-30-10\")\n",
    "print(f\"Embedding Method: CLS3 Concatenation\")\n",
    "print(f\"Classifier: LightGBM (DART)\")\n",
    "\n",
    "# Detailed metrics\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nAccuracy: {accuracy_score(test_labels, test_preds)*100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(test_labels, test_preds)*100:.2f}%\") \n",
    "print(f\"Recall: {recall_score(test_labels, test_preds)*100:.2f}%\")\n",
    "print(f\"F1-Score: {f1_score(test_labels, test_preds)*100:.2f}%\")\n",
    "\n",
    "# Sample predictions\n",
    "print(\"\\n=== Prediction Samples ===\")\n",
    "print(results_df.sample(5, random_state=42)[\n",
    "    ['text', 'true_label', 'predicted_label', 'confidence']\n",
    "].to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Memory Cleanup\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
